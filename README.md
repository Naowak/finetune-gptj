# finetune-gptj : Training Transformers with DeepSpeed

## Introduction

This repository contains the code and scripts for training transformer models on large text datasets using DeepSpeed. It's designed to be efficient and scalable, making it ideal for training large models such as GPT variants. This repository is a part of a larger project called **GPTJ-Overton**. To fully understand how to utilize this repo effectively, please refer to the GPTJ-Overton [repository](https://github.com/Naowak/gptj-overton).

## Author

ðŸ‘¤ **Yannis Bendi-Ouis (Naowak)**

- Website: [naowak.fr](https://www.naowak.fr/)
- More about GPTJ-Overton: [naowak.fr/gptj-overton](https://www.naowak.fr/gptj-overton/)

## Prerequisites

- Python 3.x
- DeepSpeed
- PyTorch
- CUDA for GPU acceleration (optional but recommended)
- Large text dataset

## License

Distributed under the MIT License with an Acknowledgment Clause. See `LICENSE` for more information.

## Contact

Yannis Bendi-Ouis - [https://www.naowak.fr](https://www.naowak.fr/)

Github : [https://github.com/Naowak/(https://www.github.com/Naowak)
