------------------


    1  git clone https://github.com/Xirider/finetune-gpt2xl.git
    2  chmod -R 777 finetune-gpt2xl/
    3  cd finetune-gpt2xl/
    4  pip install git+https://github.com/StellaAthena/transformers
    5  pip install datasets=1.8.0
    6  pip install dataset=s=1.8.0
    7  pip install datasets==1.8.0
    8  pip install deepspeed==0.6.5
    9  deepspeed --num_gpus=1 run_clm.py --deepspeed ds_config.json --model_name_or_path Cedille/fr-boris --train_file train.csv --validation_file validation.csv --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy="steps" --output_dir finetuned --eval_steps 200 --num_train_epochs 1 --gradient_accumulation_steps 2 \
   10  deepspeed --num_gpus=2 run_clm.py --deepspeed ds_config.json --model_name_or_path Cedille/fr-boris --train_file train.csv --validation_file validation.csv --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy="steps" --output_dir finetuned --eval_steps 200 --num_train_epochs 1 --gradient_accumulation_steps 2 --per_device_train_batch_size 8
   11  ls
   12  pip install -r requirements.txt 
   13  pip uninstall wandb
   14  nano requirements.txt 
   15  pip -r install requirements.txt 
   16  pip instal -r requirements.txt 
   17  pip install -r requirements.txt 
   18  pip install git+https://github.com/StellaAthena/transformers
   19  deepspeed --num_gpus=2 run_clm.py --deepspeed ds_config.json --model_name_or_path Cedille/fr-boris --train_file train.csv --validation_file validation.csv --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy="steps" --output_dir finetuned --eval_steps 200 --num_train_epochs 1 --gradient_accumulation_steps 2 --per_device_train_batch_size 8
   20  history

--------------

======================================
Welcome to the Google Deep Learning VM
======================================

Version: pytorch-gpu.1-7.m65
Based on: Debian GNU/Linux 10 (buster) (GNU/Linux 4.19.0-14-cloud-amd64 x86_64\n)

Resources:
 * Google Deep Learning Platform StackOverflow: https://stackoverflow.com/questions/tagged/google-dl-platform
 * Google Cloud Documentation: https://cloud.google.com/deep-learning-vm
 * Google Group: https://groups.google.com/forum/#!forum/google-dl-platform

To reinstall Nvidia driver (if needed) run:
sudo /opt/deeplearning/install-driver.sh
Linux gpuserver 4.19.0-14-cloud-amd64 #1 SMP Debian 4.19.171-2 (2021-01-30) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
yannis_bendiouis@gpuserver:~$ git clone https://github.com/Xirider/finetune-gpt2xl.git
Cloning into 'finetune-gpt2xl'...
remote: Enumerating objects: 384, done.
remote: Counting objects: 100% (61/61), done.
remote: Compressing objects: 100% (57/57), done.
remote: Total 384 (delta 12), reused 4 (delta 4), pack-reused 323
Receiving objects: 100% (384/384), 5.57 MiB | 21.61 MiB/s, done.
Resolving deltas: 100% (240/240), done.
yannis_bendiouis@gpuserver:~$ chmod -R 777 finetune-gpt2xl/
yannis_bendiouis@gpuserver:~$ cd finetune-gpt2xl/
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install git+https://github.com/StellaAthena/transformers
Collecting git+https://github.com/StellaAthena/transformers
  Cloning https://github.com/StellaAthena/transformers to /tmp/pip-req-build-2emt2nyn
  Running command git clone -q https://github.com/StellaAthena/transformers /tmp/pip-req-build-2emt2nyn
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (2.25.1)
Collecting filelock
  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (2020.11.13)
Collecting tokenizers!=0.11.3,<0.13,>=0.11.1
  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)
     |████████████████████████████████| 6.6 MB 1.6 MB/s 
Collecting huggingface-hub<1.0,>=0.1.0
  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)
     |████████████████████████████████| 190 kB 89.9 MB/s 
Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (4.58.0)
Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (5.4.1)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (1.19.5)
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (20.9)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (3.7.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.0.dev0) (3.7.4.3)
Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.20.0.dev0) (2.4.7)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.20.0.dev0) (3.4.0)
Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (2.10)
Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (4.0.0)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (2020.12.5)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (1.26.3)
Building wheels for collected packages: transformers
  Building wheel for transformers (PEP 517) ... done
  Created wheel for transformers: filename=transformers-4.20.0.dev0-py3-none-any.whl size=4343112 sha256=84cc8d74e1e4725572d8ad3db5af883834c69f75afd82c2922aad0e20ca74903
  Stored in directory: /tmp/pip-ephem-wheel-cache-j8q0gdlp/wheels/29/0d/8b/5691e9c97db28c672345f2b5fd136533f8f068b7e2677491ac
Successfully built transformers
Installing collected packages: filelock, tokenizers, huggingface-hub, transformers
Successfully installed filelock-3.9.0 huggingface-hub-0.12.1 tokenizers-0.12.1 transformers-4.20.0.dev0
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install datasets=1.8.0
ERROR: Invalid requirement: 'datasets=1.8.0'
Hint: = is not a valid operator. Did you mean == ?
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install dataset=s=1.8.0
ERROR: Invalid requirement: 'dataset=s=1.8.0'
Hint: = is not a valid operator. Did you mean == ?
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install datasets==1.8.0
Collecting datasets==1.8.0
  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)
     |████████████████████████████████| 237 kB 1.6 MB/s 
Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (3.0.0)
Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (20.9)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (1.19.5)
Collecting huggingface-hub<0.1.0
  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)
     |████████████████████████████████| 56 kB 6.2 MB/s 
Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (0.8.7)
Collecting multiprocess
  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)
     |████████████████████████████████| 115 kB 17.8 MB/s 
Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (2.25.1)
Collecting xxhash
  Downloading xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)
     |████████████████████████████████| 213 kB 23.1 MB/s 
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (3.7.0)
Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (1.2.2)
Collecting tqdm<4.50.0,>=4.27
  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)
     |████████████████████████████████| 69 kB 11.3 MB/s 
Collecting dill
  Downloading dill-0.3.6-py3-none-any.whl (110 kB)
     |████████████████████████████████| 110 kB 36.4 MB/s 
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.9.0)
Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (5.4.1)
Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.7.4.3)
Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets==1.8.0) (2.4.7)
Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (2.10)
Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (4.0.0)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (2020.12.5)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (1.26.3)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.8.0) (3.4.0)
Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.8.0) (2.8.1)
Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.8.0) (2021.1)
Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.8.0) (1.15.0)
Installing collected packages: tqdm, dill, xxhash, multiprocess, huggingface-hub, datasets
  Attempting uninstall: tqdm
    Found existing installation: tqdm 4.58.0
    Uninstalling tqdm-4.58.0:
      Successfully uninstalled tqdm-4.58.0
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.12.1
    Uninstalling huggingface-hub-0.12.1:
      Successfully uninstalled huggingface-hub-0.12.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
transformers 4.20.0.dev0 requires huggingface-hub<1.0,>=0.1.0, but you have huggingface-hub 0.0.19 which is incompatible.
Successfully installed datasets-1.8.0 dill-0.3.6 huggingface-hub-0.0.19 multiprocess-0.70.14 tqdm-4.49.0 xxhash-3.2.0
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install deepspeed==0.6.5
Collecting deepspeed==0.6.5
  Downloading deepspeed-0.6.5.tar.gz (567 kB)
     |████████████████████████████████| 567 kB 1.6 MB/s 
Collecting hjson
  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)
     |████████████████████████████████| 54 kB 5.1 MB/s 
Collecting ninja
  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)
     |████████████████████████████████| 145 kB 67.9 MB/s 
Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5) (1.19.5)
Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5) (20.9)
Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5) (5.8.0)
Collecting py-cpuinfo
  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5) (1.7.1)
Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5) (4.49.0)
Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->deepspeed==0.6.5) (2.4.7)
Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->deepspeed==0.6.5) (3.7.4.3)
Building wheels for collected packages: deepspeed
  Building wheel for deepspeed (setup.py) ... done
  Created wheel for deepspeed: filename=deepspeed-0.6.5-py3-none-any.whl size=563631 sha256=0009583a84f3727d1b140418f273747dd6601bd509e15382bf705536384b5d4f
  Stored in directory: /home/yannis_bendiouis/.cache/pip/wheels/da/81/aa/058ca61d1510a7144f1ea38897e04c75f5a88f77be3b1c4e7d
Successfully built deepspeed
Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed
Successfully installed deepspeed-0.6.5 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ deepspeed --num_gpus=1 run_clm.py \
> --deepspeed ds_config.json \
> --model_name_or_path Cedille/fr-boris \
> --train_file train.csv \
> --validation_file validation.csv \
> --do_train \
> --do_eval \
> --fp16 \
> --overwrite_cache \
> --evaluation_strategy="steps" \
> --output_dir finetuned \
> --eval_steps 200 \
> --num_train_epochs 1 \
> --gradient_accumulation_steps 2 \
> --per_device_train_batch_size 8^C
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ deepspeed --num_gpus=2 run_clm.py \
> --deepspeed ds_config.json \
> --model_name_or_path Cedille/fr-boris \
> --train_file train.csv \
> --validation_file validation.csv \
> --do_train \
> --do_eval \
> --fp16 \
> --overwrite_cache \
> --evaluation_strategy="steps" \
> --output_dir finetuned \
> --eval_steps 200 \
> --num_train_epochs 1 \
> --gradient_accumulation_steps 2 \
> --per_device_train_batch_size 8
[2023-02-17 13:39:12,045] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2023-02-17 13:39:12,067] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-02-17 13:39:14,643] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.7 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 run_clm.py --deepspeed ds_config.json --model_name_or_path Cedille/fr-boris --train_file train.csv --validation_file validation.csv --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=steps --output_dir finetuned --eval_steps 200 --num_train_epochs 1 --gradient_accumulation_steps 2 --per_device_train_batch_size 8
[2023-02-17 13:39:15,336] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2023-02-17 13:39:15,358] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-02-17 13:39:15,358] [INFO] [launch.py:110:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-02-17 13:39:15,358] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-02-17 13:39:15,358] [INFO] [launch.py:123:main] dist_world_size=2
[2023-02-17 13:39:15,358] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1
Traceback (most recent call last):
  File "run_clm.py", line 33, in <module>
    import transformers
  File "/opt/conda/lib/python3.7/site-packages/transformers/__init__.py", line 30, in <module>
    from . import dependency_versions_check
  File "/opt/conda/lib/python3.7/site-packages/transformers/dependency_versions_check.py", line 17, in <module>
    from .utils.versions import require_version, require_version_core
  File "/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py", line 46, in <module>
    from .hub import (
  File "/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py", line 39, in <module>
    from huggingface_hub import HfFolder, Repository, create_repo, list_repo_files, whoami
ImportError: cannot import name 'create_repo' from 'huggingface_hub' (/opt/conda/lib/python3.7/site-packages/huggingface_hub/__init__.py)
Traceback (most recent call last):
  File "run_clm.py", line 33, in <module>
    import transformers
  File "/opt/conda/lib/python3.7/site-packages/transformers/__init__.py", line 30, in <module>
    from . import dependency_versions_check
  File "/opt/conda/lib/python3.7/site-packages/transformers/dependency_versions_check.py", line 17, in <module>
    from .utils.versions import require_version, require_version_core
  File "/opt/conda/lib/python3.7/site-packages/transformers/utils/__init__.py", line 46, in <module>
    from .hub import (
  File "/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py", line 39, in <module>
    from huggingface_hub import HfFolder, Repository, create_repo, list_repo_files, whoami
ImportError: cannot import name 'create_repo' from 'huggingface_hub' (/opt/conda/lib/python3.7/site-packages/huggingface_hub/__init__.py)
[2023-02-17 13:39:17,371] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 9708
[2023-02-17 13:39:17,371] [INFO] [launch.py:178:sigkill_handler] Killing subprocess 9709
[2023-02-17 13:39:17,372] [ERROR] [launch.py:184:sigkill_handler] ['/opt/conda/bin/python3.7', '-u', 'run_clm.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--model_name_or_path', 'Cedille/fr-boris', '--train_file', 'train.csv', '--validation_file', 'validation.csv', '--do_train', '--do_eval', '--fp16', '--overwrite_cache', '--evaluation_strategy=steps', '--output_dir', 'finetuned', '--eval_steps', '200', '--num_train_epochs', '1', '--gradient_accumulation_steps', '2', '--per_device_train_batch_size', '8'] exits with return code = 1
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ ls
LICENSE.MD      ds_config_gptneo.json  run_generate_batches.py  text2csv.py  validation.csv
README.md       requirements.txt       run_generate_neo.py      train.csv    validation.txt
ds_config.json  run_clm.py             run_generation.py        train.txt
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install -r requirements.txt 
Collecting transformers==4.7.0
  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)
     |████████████████████████████████| 2.5 MB 1.6 MB/s 
Requirement already satisfied: datasets==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.8.0)
Collecting wandb
  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)
     |████████████████████████████████| 2.0 MB 94.3 MB/s 
Requirement already satisfied: deepspeed in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.6.5)
Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (3.2.0)
Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (3.0.0)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (1.19.5)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (3.7.0)
Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.70.14)
Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.3.6)
Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (20.9)
Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (1.2.2)
Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.0.19)
Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (2.25.1)
Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.8.7)
Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (4.49.0)
Collecting sacremoses
  Downloading sacremoses-0.0.53.tar.gz (880 kB)
     |████████████████████████████████| 880 kB 98.5 MB/s 
^CERROR: Operation cancelled by user
^C
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ ^C
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip uninstall wandb
WARNING: Skipping wandb as it is not installed.
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ nano requirements.txt 
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip -r install requirements.txt 

Usage:   
  pip <command> [options]

no such option: -r
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip instal -r requirements.txt 
ERROR: unknown command "instal" - maybe you meant "install"
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install -r requirements.txt 
Collecting transformers==4.7.0
  Using cached transformers-4.7.0-py3-none-any.whl (2.5 MB)
Requirement already satisfied: datasets==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.8.0)
Requirement already satisfied: deepspeed==0.6.5 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.6.5)
Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (1.2.2)
Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (2.25.1)
Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (20.9)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (3.7.0)
Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (4.49.0)
Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.8.7)
Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.3.6)
Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (3.2.0)
Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.0.19)
Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (3.0.0)
Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (0.70.14)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0->-r requirements.txt (line 2)) (1.19.5)
Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5->-r requirements.txt (line 3)) (5.8.0)
Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5->-r requirements.txt (line 3)) (9.0.0)
Requirement already satisfied: ninja in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5->-r requirements.txt (line 3)) (1.11.1)
Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5->-r requirements.txt (line 3)) (1.7.1)
Requirement already satisfied: hjson in /opt/conda/lib/python3.7/site-packages (from deepspeed==0.6.5->-r requirements.txt (line 3)) (3.1.0)
Collecting huggingface-hub<0.1.0
  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.7.0->-r requirements.txt (line 1)) (2020.11.13)
Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from transformers==4.7.0->-r requirements.txt (line 1)) (5.4.1)
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.7.0->-r requirements.txt (line 1)) (3.9.0)
Collecting sacremoses
  Using cached sacremoses-0.0.53.tar.gz (880 kB)
Collecting tokenizers<0.11,>=0.10.1
  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)
     |████████████████████████████████| 3.3 MB 2.7 MB/s 
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0->-r requirements.txt (line 2)) (2020.12.5)
Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0->-r requirements.txt (line 2)) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0->-r requirements.txt (line 2)) (2.10)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0->-r requirements.txt (line 2)) (1.26.3)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.8.0->-r requirements.txt (line 2)) (3.4.0)
Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.8.0->-r requirements.txt (line 2)) (3.7.4.3)
Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets==1.8.0->-r requirements.txt (line 2)) (2.4.7)
Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.8.0->-r requirements.txt (line 2)) (2.8.1)
Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.8.0->-r requirements.txt (line 2)) (2021.1)
Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.8.0->-r requirements.txt (line 2)) (1.15.0)
Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.7.0->-r requirements.txt (line 1)) (7.1.2)
Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.7.0->-r requirements.txt (line 1)) (1.0.1)
Building wheels for collected packages: sacremoses
  Building wheel for sacremoses (setup.py) ... done
  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=1162b47b67daa8b96357fff94fda6af69a681d94cd8ed83b7aa73d91eef4598e
  Stored in directory: /home/yannis_bendiouis/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9
Successfully built sacremoses
Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.12.1
    Uninstalling tokenizers-0.12.1:
      Successfully uninstalled tokenizers-0.12.1
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.0.19
    Uninstalling huggingface-hub-0.0.19:
      Successfully uninstalled huggingface-hub-0.0.19
  Attempting uninstall: transformers
    Found existing installation: transformers 4.20.0.dev0
    Uninstalling transformers-4.20.0.dev0:
      Successfully uninstalled transformers-4.20.0.dev0
Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.7.0
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ pip install git+https://github.com/StellaAthena/transformers
Collecting git+https://github.com/StellaAthena/transformers
  Cloning https://github.com/StellaAthena/transformers to /tmp/pip-req-build-rkrc35om
  Running command git clone -q https://github.com/StellaAthena/transformers /tmp/pip-req-build-rkrc35om
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (3.9.0)
Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (1.19.5)
Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (20.9)
Collecting tokenizers!=0.11.3,<0.13,>=0.11.1
  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)
Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (4.49.0)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (2020.11.13)
Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (3.7.0)
Collecting huggingface-hub<1.0,>=0.1.0
  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)
Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (5.4.1)
Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.0.dev0) (2.25.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.0.dev0) (3.7.4.3)
Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.20.0.dev0) (2.4.7)
Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.20.0.dev0) (3.4.0)
Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (4.0.0)
Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (2.10)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (1.26.3)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.20.0.dev0) (2020.12.5)
Building wheels for collected packages: transformers
  Building wheel for transformers (PEP 517) ... done
  Created wheel for transformers: filename=transformers-4.20.0.dev0-py3-none-any.whl size=4343112 sha256=51133507527201a3232251ec2a0f2bbfdaaf0553d3a1c20af987b6b9844d7e9c
  Stored in directory: /tmp/pip-ephem-wheel-cache-9va44yxq/wheels/29/0d/8b/5691e9c97db28c672345f2b5fd136533f8f068b7e2677491ac
Successfully built transformers
Installing collected packages: tokenizers, huggingface-hub, transformers
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.10.3
    Uninstalling tokenizers-0.10.3:
      Successfully uninstalled tokenizers-0.10.3
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.0.8
    Uninstalling huggingface-hub-0.0.8:
      Successfully uninstalled huggingface-hub-0.0.8
  Attempting uninstall: transformers
    Found existing installation: transformers 4.7.0
    Uninstalling transformers-4.7.0:
      Successfully uninstalled transformers-4.7.0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
datasets 1.8.0 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.12.1 which is incompatible.
Successfully installed huggingface-hub-0.12.1 tokenizers-0.12.1 transformers-4.20.0.dev0
yannis_bendiouis@gpuserver:~/finetune-gpt2xl$ deepspeed --num_gpus=2 run_clm.py \
> --deepspeed ds_config.json \
> --model_name_or_path Cedille/fr-boris \
> --train_file train.csv \
> --validation_file validation.csv \
> --do_train \
> --do_eval \
> --fp16 \
> --overwrite_cache \
> --evaluation_strategy="steps" \
> --output_dir finetuned \
> --eval_steps 200 \
> --num_train_epochs 1 \
> --gradient_accumulation_steps 2 \
> --per_device_train_batch_size 8
[2023-02-17 13:41:41,504] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2023-02-17 13:41:41,526] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-02-17 13:41:43,208] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.7 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 run_clm.py --deepspeed ds_config.json --model_name_or_path Cedille/fr-boris --train_file train.csv --validation_file validation.csv --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=steps --output_dir finetuned --eval_steps 200 --num_train_epochs 1 --gradient_accumulation_steps 2 --per_device_train_batch_size 8
[2023-02-17 13:41:43,897] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2023-02-17 13:41:43,918] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2023-02-17 13:41:43,918] [INFO] [launch.py:110:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-02-17 13:41:43,918] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-02-17 13:41:43,918] [INFO] [launch.py:123:main] dist_world_size=2
[2023-02-17 13:41:43,918] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2023-02-17 13:41:46,147] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2023-02-17 13:41:46,148] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.
[2023-02-17 13:41:46,165] [INFO] [distributed.py:49:init_distributed] Initializing torch distributed with backend: nccl
02/17/2023 13:41:53 - WARNING - __main__ -   Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
02/17/2023 13:41:53 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
02/17/2023 13:41:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=ds_config.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=200,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
full_determinism=False,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=finetuned/runs/Feb17_13-41-46_gpuserver,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=finetuned,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=finetuned,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
02/17/2023 13:41:54 - WARNING - datasets.builder -   Using custom data configuration default-9bb97ba1aabfc043
Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/yannis_bendiouis/.cache/huggingface/datasets/csv/default-9bb97ba1aabfc043/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...
02/17/2023 13:41:54 - WARNING - datasets.builder -   Using custom data configuration default-9bb97ba1aabfc043
Dataset csv downloaded and prepared to /home/yannis_bendiouis/.cache/huggingface/datasets/csv/default-9bb97ba1aabfc043/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.
02/17/2023 13:41:54 - WARNING - datasets.builder -   Reusing dataset csv (/home/yannis_bendiouis/.cache/huggingface/datasets/csv/default-9bb97ba1aabfc043/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)
Downloading: 100%|█████████████████████████████████████████████████████████████| 944/944 [00:00<00:00, 969kB/s]
[INFO|configuration_utils.py:659] 2023-02-17 13:41:56,123 >> loading configuration file https://huggingface.co/Cedille/fr-boris/resolve/main/config.json from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/056dd57a7e9f7656c4fae0b440e3edfafbcf91cd7fd4f0b66929465572c85c8b.1dbb2697307e377e14193777e7131a9011cdfe5daf541e8bdbc7898d5ec67b0e
[INFO|configuration_utils.py:708] 2023-02-17 13:41:56,124 >> Model config GPTJConfig {
  "_name_or_path": "Cedille/fr-boris",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTJForCausalLM"
  ],
  "attn_pdrop": 0.0,
  "bos_token_id": 50256,
  "embd_pdrop": 0.0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gptj",
  "n_ctx": 2048,
  "n_embd": 4096,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 28,
  "n_positions": 2048,
  "resid_pdrop": 0.0,
  "rotary_dim": 64,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 1.0
    }
  },
  "tie_word_embeddings": false,
  "tokenizer_class": "GPT2Tokenizer",
  "torch_dtype": "float32",
  "transformers_version": "4.20.0.dev0",
  "use_cache": true,
  "vocab_size": 50400
}

Downloading: 100%|█████████████████████████████████████████████████████████████| 236/236 [00:00<00:00, 233kB/s]
Downloading: 100%|██████████████████████████████████████████████████████████| 779k/779k [00:00<00:00, 1.09MB/s]
Downloading: 100%|███████████████████████████████████████████████████████████| 446k/446k [00:00<00:00, 507kB/s]
Downloading: 100%|████████████████████████████████████████████████████████| 1.29M/1.29M [00:01<00:00, 1.24MB/s]
[INFO|hub.py:583] 2023-02-17 13:42:07,250 >> https://huggingface.co/Cedille/fr-boris/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /home/yannis_bendiouis/.cache/huggingface/transformers/tmpmxbb23tt
Downloading: 100%|██████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 80.5kB/s]
[INFO|hub.py:587] 2023-02-17 13:42:07,996 >> storing https://huggingface.co/Cedille/fr-boris/resolve/main/special_tokens_map.json in cache at /home/yannis_bendiouis/.cache/huggingface/transformers/a9ac7ebb969c9f5b7568ffa57ef0b0d84b4297b21e365b9a2ac2adb9a6ee8c5e.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22
[INFO|hub.py:595] 2023-02-17 13:42:07,996 >> creating metadata file for /home/yannis_bendiouis/.cache/huggingface/transformers/a9ac7ebb969c9f5b7568ffa57ef0b0d84b4297b21e365b9a2ac2adb9a6ee8c5e.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22
[INFO|tokenization_utils_base.py:1781] 2023-02-17 13:42:08,743 >> loading file https://huggingface.co/Cedille/fr-boris/resolve/main/vocab.json from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/4f9215815f3d89f7eab745660399c1a71405e767103b5a74ee57b3db60076a20.a1b97b074a5ac71fad0544c8abc1b3581803d73832476184bde6cff06a67b6bb
[INFO|tokenization_utils_base.py:1781] 2023-02-17 13:42:08,743 >> loading file https://huggingface.co/Cedille/fr-boris/resolve/main/merges.txt from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/0072ec227f0bfe26269dc232b82b95ac548a3ffcaec7e421ac85be22da181e40.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435
[INFO|tokenization_utils_base.py:1781] 2023-02-17 13:42:08,743 >> loading file https://huggingface.co/Cedille/fr-boris/resolve/main/tokenizer.json from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/18d9618a9d0c866b3eadeb1576a771f2d2e8a2e908a7e233e5d91463187de78c.c83461319bb31d7584a5150318794d1f904cdcc960158c8c411bf05676b432c8
[INFO|tokenization_utils_base.py:1781] 2023-02-17 13:42:08,743 >> loading file https://huggingface.co/Cedille/fr-boris/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1781] 2023-02-17 13:42:08,743 >> loading file https://huggingface.co/Cedille/fr-boris/resolve/main/special_tokens_map.json from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/a9ac7ebb969c9f5b7568ffa57ef0b0d84b4297b21e365b9a2ac2adb9a6ee8c5e.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22
[INFO|tokenization_utils_base.py:1781] 2023-02-17 13:42:08,743 >> loading file https://huggingface.co/Cedille/fr-boris/resolve/main/tokenizer_config.json from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/9a199191423549676dad36c3d18ec9a2bfb53d6dc1eb57eb2b173a6961417d4b.1ed40b8bbfcaa6c8f0a9eb550bd5dbaea6cd508f07d9966eb2d08cb87997d132
[INFO|hub.py:583] 2023-02-17 13:42:09,569 >> https://huggingface.co/Cedille/fr-boris/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/yannis_bendiouis/.cache/huggingface/transformers/tmpg1weg4hx
Downloading: 100%|████████████████████████████████████████████████████████| 22.7G/22.7G [18:53<00:00, 21.4MB/s]
[INFO|hub.py:587] 2023-02-17 14:01:04,520 >> storing https://huggingface.co/Cedille/fr-boris/resolve/main/pytorch_model.bin in cache at /home/yannis_bendiouis/.cache/huggingface/transformers/bbc8f7176e305325362028f18d2cc01b57a04fbffdba4fee8828a2552aa8b221.1ee53647746d88a431298deaedcb526d0b35e69b4b2b16547c145f7d38ff67f6
[INFO|hub.py:595] 2023-02-17 14:01:04,520 >> creating metadata file for /home/yannis_bendiouis/.cache/huggingface/transformers/bbc8f7176e305325362028f18d2cc01b57a04fbffdba4fee8828a2552aa8b221.1ee53647746d88a431298deaedcb526d0b35e69b4b2b16547c145f7d38ff67f6
[INFO|modeling_utils.py:2049] 2023-02-17 14:01:04,520 >> loading weights file https://huggingface.co/Cedille/fr-boris/resolve/main/pytorch_model.bin from cache at /home/yannis_bendiouis/.cache/huggingface/transformers/bbc8f7176e305325362028f18d2cc01b57a04fbffdba4fee8828a2552aa8b221.1ee53647746d88a431298deaedcb526d0b35e69b4b2b16547c145f7d38ff67f6
[INFO|modeling_utils.py:2425] 2023-02-17 14:01:55,345 >> All model checkpoint weights were used when initializing GPTJForCausalLM.

[INFO|modeling_utils.py:2434] 2023-02-17 14:01:55,345 >> All the weights of GPTJForCausalLM were initialized from the model checkpoint at Cedille/fr-boris.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTJForCausalLM for predictions without further training.
02/17/2023 14:02:03 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fb7f9b8c200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|                                                                                    | 0/1 [00:00<?, ?ba/s]02/17/2023 14:02:03 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f412cf9e290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|                                                                                    | 0/1 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3431] 2023-02-17 14:02:07,827 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1462828 > 1024). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (1462828 > 1024). Running this sequence through the model will result in indexing errors
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.64s/ba]
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.54s/ba]
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.43ba/s]
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 69.56ba/s]
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.15s/ba]
100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 193.77ba/s]
[INFO|trainer.py:506] 2023-02-17 14:02:09,376 >> Using amp half precision backend
[2023-02-17 14:02:09,381] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.24s/ba]
100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 172.97ba/s]
[2023-02-17 14:02:15,907] [INFO] [engine.py:279:__init__] DeepSpeed Flops Profiler Enabled: False
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/yannis_bendiouis/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /home/yannis_bendiouis/.cache/torch_extensions/cpu_adam...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/yannis_bendiouis/.cache/torch_extensions as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yannis_bendiouis/.cache/torch_extensions/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[1/3] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /opt/conda/lib/python3.7/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o 
[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /opt/conda/lib/python3.7/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 25.446550846099854 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 25.449947834014893 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2023-02-17 14:02:44,260] [INFO] [engine.py:1102:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2023-02-17 14:02:44,280] [INFO] [engine.py:1109:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-02-17 14:02:44,280] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-02-17 14:02:44,280] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2023-02-17 14:02:44,280] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 200000000.0
[2023-02-17 14:02:44,280] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 200000000.0
[2023-02-17 14:02:44,280] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-02-17 14:02:44,280] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Using /home/yannis_bendiouis/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /home/yannis_bendiouis/.cache/torch_extensions/utils...
Using /home/yannis_bendiouis/.cache/torch_extensions as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Emitting ninja build file /home/yannis_bendiouis/.cache/torch_extensions/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.7/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o 
[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so
Loading extension module utils...
Time to load utils op: 9.706175088882446 seconds
Loading extension module utils...
Time to load utils op: 9.72064757347107 seconds
Rank: 0 partition count [2] and sizes[(3024855594, False)] 
Rank: 1 partition count [2] and sizes[(3024855594, False)] 
[2023-02-17 14:03:05,586] [INFO] [utils.py:828:see_memory_usage] Before initializing optimizer states
[2023-02-17 14:03:05,588] [INFO] [utils.py:833:see_memory_usage] MA 11.76 GB         Max_MA 11.76 GB         CA 22.66 GB         Max_CA 23 GB 
[2023-02-17 14:03:05,589] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 39.41 GB, percent = 23.6%
Using /home/yannis_bendiouis/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0015320777893066406 seconds
[2023-02-17 14:03:14,595] [INFO] [utils.py:828:see_memory_usage] After initializing optimizer states
[2023-02-17 14:03:14,596] [INFO] [utils.py:833:see_memory_usage] MA 11.76 GB         Max_MA 11.76 GB         CA 22.66 GB         Max_CA 23 GB 
[2023-02-17 14:03:14,596] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 102.08 GB, percent = 61.0%
[2023-02-17 14:03:14,596] [INFO] [stage_1_and_2.py:511:__init__] optimizer state initialized
[2023-02-17 14:03:14,641] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer
[2023-02-17 14:03:14,642] [INFO] [utils.py:833:see_memory_usage] MA 11.76 GB         Max_MA 11.76 GB         CA 22.66 GB         Max_CA 23 GB 
[2023-02-17 14:03:14,642] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 102.08 GB, percent = 61.0%
[2023-02-17 14:03:14,642] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2023-02-17 14:03:14,642] [INFO] [engine.py:786:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR
[2023-02-17 14:03:14,642] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb7f886e450>
[2023-02-17 14:03:14,643] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]
[2023-02-17 14:03:14,643] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   amp_enabled .................. False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   amp_params ................... False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   communication_data_type ...... None
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   curriculum_enabled ........... False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   curriculum_params ............ False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   disable_allgather ............ False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   dump_state ................... False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01
[2023-02-17 14:03:14,644] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   elasticity_enabled ........... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   fp16_enabled ................. True
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   global_rank .................. 0
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 2
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 65536
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   loss_scale ................... 0
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   memory_breakdown ............. False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   optimizer_name ............... adamw
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   pld_enabled .................. False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   pld_params ................... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   prescale_gradients ........... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_groups .............. 1
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_offset .............. 1000
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_period .............. 1000
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_rounding ............ 0
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_training_enabled .... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_type ................ 0
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   quantize_verbose ............. False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   scheduler_name ............... WarmupLR
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   sparse_attention ............. None
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False
[2023-02-17 14:03:14,645] [INFO] [config.py:1063:print]   steps_per_print .............. 2000
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   tensorboard_output_path ...... 
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   train_batch_size ............. 32
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  8
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   world_size ................... 2
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   zero_config .................. {
    "stage": 2, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 2.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 2.000000e+08, 
    "overlap_comm": true, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": false, 
    "offload_param": null, 
    "offload_optimizer": {
        "device": null, 
        "nvme_path": null, 
        "buffer_count": 4, 
        "pin_memory": false, 
        "pipeline_read": false, 
        "pipeline_write": false, 
        "fast_init": false
    }, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_16bit_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   zero_enabled ................. True
[2023-02-17 14:03:14,646] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 2
[2023-02-17 14:03:14,646] [INFO] [config.py:1071:print]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 5e-05, 
            "warmup_num_steps": 0
        }
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "cpu_offload": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 8, 
    "wall_clock_breakdown": false
}
Using /home/yannis_bendiouis/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004246234893798828 seconds
[INFO|trainer.py:1430] 2023-02-17 14:03:14,647 >> ***** Running training *****
[INFO|trainer.py:1431] 2023-02-17 14:03:14,647 >>   Num examples = 1428
[INFO|trainer.py:1432] 2023-02-17 14:03:14,647 >>   Num Epochs = 1
[INFO|trainer.py:1433] 2023-02-17 14:03:14,647 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1434] 2023-02-17 14:03:14,647 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1435] 2023-02-17 14:03:14,647 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1436] 2023-02-17 14:03:14,647 >>   Total optimization steps = 45
  2%|█▋                                                                         | 1/45 [00:27<20:19, 27.73s/it][2023-02-17 14:03:52,193] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 65536
  4%|███▎                                                                       | 2/45 [00:37<16:01, 22.35s/it][2023-02-17 14:04:01,151] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768.0
 11%|████████▎                                                                  | 5/45 [01:11<10:19, 15.48s/it][2023-02-17 14:04:35,641] [INFO] [stage_1_and_2.py:1675:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
100%|██████████████████████████████████████████████████████████████████████████| 45/45 [09:36<00:00, 12.07s/it][INFO|trainer.py:1673] 2023-02-17 14:12:51,274 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 576.6273, 'train_samples_per_second': 2.476, 'train_steps_per_second': 0.078, 'train_loss': 3.6902560763888888, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████| 45/45 [09:36<00:00, 12.81s/it]
[INFO|trainer.py:2410] 2023-02-17 14:12:51,276 >> Saving model checkpoint to finetuned
[INFO|configuration_utils.py:446] 2023-02-17 14:12:51,277 >> Configuration saved in finetuned/config.json
[INFO|modeling_utils.py:1610] 2023-02-17 14:16:05,782 >> The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at finetuned/pytorch_model.bin.index.json.
[INFO|tokenization_utils_base.py:2123] 2023-02-17 14:16:05,783 >> tokenizer config file saved in finetuned/tokenizer_config.json
[INFO|tokenization_utils_base.py:2130] 2023-02-17 14:16:05,783 >> Special tokens file saved in finetuned/special_tokens_map.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     3.6903
  train_runtime            = 0:09:36.62
  train_samples            =       1428
  train_samples_per_second =      2.476
  train_steps_per_second   =      0.078
02/17/2023 14:16:05 - INFO - __main__ -   *** Evaluate ***
[INFO|trainer.py:2660] 2023-02-17 14:16:05,919 >> ***** Running Evaluation *****
[INFO|trainer.py:2662] 2023-02-17 14:16:05,919 >>   Num examples = 4
[INFO|trainer.py:2665] 2023-02-17 14:16:05,919 >>   Batch size = 8
100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.28it/s]
***** eval metrics *****
  epoch                   =        1.0
  eval_loss               =      4.793
  eval_runtime            = 0:00:00.86
  eval_samples            =          4
  eval_samples_per_second =      4.605
  eval_steps_per_second   =      1.151
  perplexity              =    120.659
[2023-02-17 14:16:11,300] [INFO] [launch.py:210:main] Process 10736 exits successfully.
[2023-02-17 14:16:12,301] [INFO] [launch.py:210:main] Process 10737 exits successfully.